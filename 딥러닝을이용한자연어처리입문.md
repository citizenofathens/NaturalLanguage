# :bulb: 이 문서는  https://wikidocs.net/book/2155 를 참고하며 이해하고 정리한 글이다.




# 소프트 맥스 회귀

+ g = sns.pairplot(data, hue="Species", palette="husl")
+ pairplot은 데이터프레임을 인수로 받아 데이터프레임의 각 열의 조합에 따라서 산점도(scatter plot)을 그린다 +
+ 즉 4개의 컬럼이 있으면 자기 자신과의 조합이면 히스토그램 (표로 되어 있는 도수 분포를 정보 그림으로 나타낸 것이다) 을 그린다
+ 

데이터프레임 정수 인코딩 
df['대상컬럼'].replace(대상컬럼리스트),(0~컬럼리스트길이)
series에는 plot 메서드도 있다 value_counts().plot()과 같이 활용 가능 


from tensorflow.keras.utils import to_categorical
데이터프레임에 대해 원-핫 인코딩

adam 경사하강법의 일종

valid_set을 가지고 가중치를 업데이트 하지는 않는다


초기퍼셉트론은 임계치 1을 넘기면 출력되는 계단 함수를 사용하였다



단계를 layer라고 부른다 



XOR게이트는 단층퍼셉트론으로 구현할 수가 없다 하나의 선형 영역으로 나눌 수가 없다


시그모이드 함수의 양 끝단에서는 기울기가 0에 가깝다. 이 값들이 앞으로 전해지면서 0애 가까운 값이 계속 곱해진다면? 기울기 소실이 발생한다
그래서 시그모이드 함수는 은닉층에서 사용되는 것이 지양되는 것이다. 


하이퍼볼릭탄젠트함수는 '입력값'을 -1에서 1로 변환한다

하이퍼볼릭 탄젠트도 그렇지만 시그모이드와 비슷한 형태여서 기울기 소실 문제가 있지만 시그모이드 함수와 달리 0을 중심으로 하여 반환값의 변화폭이 더 크므로 

증상이 덜하여 시그모이드보다는 더 쓰인다


relu 함수는 최고의 인기 함수. 

음수 이전 값은 0이고 양수를 입력하면 입력값을 그대로 반환하여 특정 양수값에 수렴하지 않으므로 시그모이드 함수보다 더 잘 작동한다  단 입력값이 음수이면 기울기가 0이 되므로 뉴런이 회생되기가 어렵다

그 문제를 dying RelU라 한다 


Leaky RelU는 죽은 Relu를 보완하기 위한 변형 함수이다 

음수일 떄의 기울기를 하이퍼파라미터로 줘서 죽지 않게 만든다 .


크로스엔트로피는 낮은 확률로 예측해서 맞추거나, 높은 확률로 예측해서 틀리는 경우 loss가 더 크다



손실 함수의 값을 줄여나가며 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라진다 

배치 경사 하강법 BGD는 에포크 한 번당 모든 매개변수 업데이트를 단 한 번 수행한다

전체 데이터를 고려하여 학습하여 에포크당 시간이 오래 걸리고 메모리를 크게 잡아먹지만 글로벌 미니멈을 찾을 수 있다


확률적 경사 하강법은 전체 데이터에 대해 계산 하면 시간이 너무 오래걸리니 매개 변수값 조정 시 전체가 아닌 랜덤으로 선택한 하나의 데이터에 대해서만 계산한다

속도는 빠르지만 정확도가 낮을 수 있다. 매개변수 변경폭이 불안정하다.(이리저리 튀며 수렴하는 경향)


미니 배치 경사 하강법 정해진 양에 대해서만 계산한다

정해진 양에 대해서만 계산하여 매개 변수의 값을 조정 전체 데이터 계산 보다 빠르고 SGD보다 안정적이므로 실제로 가장 많이 사용된다 



모멘텀은 계산된 기울기 한 시점 이전 접선의 기울기값을 일정비율만큼 반영하여 공이 내려올 때 관성의 힘으로 넘어가는 효과를 줄 수 있다. 글로벌 미니멈으로 수렴하도록


아다그라드 

각 매개변수에 서로 다른 학습률을 적용하는 함수이다  변화가 많으면 학습률을 작게 , 적으면 학습률을 높게 설정한다 


RMSprop

아다그라드를 보완하여 나중에 학습률 떨어지는 점을 다른 수식으로 단점을 개선하였다

아담 

RMSprop과 모멘텀을 합친 듯하여 방향과 학습률을 모두 잡기 위한다


배치 크기

설명이 너무 적절하여 그대로 가져온다
배치 크기는 몇 개의 데이터 단위로 매개변수를 업데이트 하는지를 말합니다. 
현실에 비유하면 문제지에서 몇 개씩 문제를 풀고나서 정답지를 확인하느냐의 문제입니다.
사람은 문제를 풀고 정답을 보는 순간 부족했던 점을 깨달으며 지식이 업데이트 된다고 하였습니다.

배치 크기 만큼 학습데이터가 몇 번씩 들어가고 나서 처음부터 업데이트를 순차적으로 진행할 것인데
배치 크ꈰ가 이 배치 크기보다 작다면 업데이트를 하고 바로 그 업데이트된 데이터로 학습 하고 또 업데이트를 하게 될 것이다.
이 간격을 잘 조정해줄 필요가 있다 

